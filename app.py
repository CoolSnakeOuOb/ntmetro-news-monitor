import streamlit as st
import feedparser
from datetime import datetime, timedelta
import time
from urllib.parse import quote_plus, urlparse
import asyncio
import platform
import multiprocessing
import streamlit.components.v1 as components

# For Windows, set event loop policy for Playwright
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

CUTOFF_HOURS = 24

def is_within_24_hours(pub_time_str):
    try:
        pub_time = datetime.strptime(pub_time_str, "%a, %d %b %Y %H:%M:%S %Z")
        return datetime.utcnow() - pub_time <= timedelta(hours=CUTOFF_HOURS)
    except ValueError:
        try:
            pub_time = datetime.strptime(pub_time_str, "%a, %d %b %Y %H:%M:%S %z")
            return datetime.utcnow().replace(tzinfo=None) - pub_time.replace(tzinfo=None) <= timedelta(hours=CUTOFF_HOURS)
        except:
            return False
    except:
        return False

# Playwright è§£æé‚è¼¯
def playwright_worker(url: str, return_dict):
    from playwright.sync_api import sync_playwright
    if platform.system() == "Windows":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    browser = None
    final_url = url
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=20000)
            page.wait_for_timeout(2000)
            final_url = page.url
    except:
        final_url = url
    finally:
        return_dict['final_url'] = final_url
        if browser:
            try: browser.close()
            except: pass

# RSS æŠ“å–
def fetch_news_for_query(query):
    encoded_query = quote_plus(query)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    feed = feedparser.parse(rss_url)
    results = []
    for entry in feed.entries:
        if hasattr(entry, 'published') and is_within_24_hours(entry.published):
            entry_url = entry.links[0]["href"] if entry.links else entry.link
            results.append({
                "label": query,
                "title": entry.title,
                "url": entry_url,
                "published": entry.published
            })
    return results

# åˆ†ç¾¤
def group_by_label(news_items):
    grouped = {}
    for item in news_items:
        grouped.setdefault(item["label"], []).append(item)
    return grouped

# --- Streamlit UI ---
st.set_page_config(page_title="æ·é‹è¼¿æƒ…å·¥å…· v4.1", page_icon="ğŸš‡", layout="wide")
st.title("ğŸ“° æ–°åŒ—æ·é‹è¼¿æƒ…å·¥å…·")

#ä½¿ç”¨èªªæ˜
with st.expander("ğŸ“– ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1. **è¼¸å…¥é—œéµå­—**
    2. é»æ“Š **ğŸ“¥ æŠ“å–æ–°è** å–å¾— 24 å°æ™‚å…§æ–°è
    3. åœ¨æ–°èåˆ—è¡¨ä¸­**å‹¾é¸æ¬²åŒ¯å‡ºä¹‹æ–°è**
    4. æŒ‰ä¸‹ **ğŸ“¤ ç”¢ç”Ÿ LINE è¨Šæ¯**
    5. é»æ“Š **ğŸ“‹ è¤‡è£½åˆ°å‰ªè²¼ç°¿**ï¼Œå°‡å…§å®¹è²¼åˆ° LINE ç¾¤çµ„
    """)

# åˆå§‹åŒ– session_state
if 'news_results' not in st.session_state:
    st.session_state.news_results = []
if 'resolved_url_cache' not in st.session_state:
    st.session_state.resolved_url_cache = {}
if 'form_submitted' not in st.session_state:
    st.session_state.form_submitted = False

default_keywords = "æ·é‹, è¼•è»Œ, ç’°ç‹€ç·š, æ–°åŒ—, è»Œé“, éµè·¯"
keywords_input = st.text_input("ğŸ” è«‹è¼¸å…¥é—œéµå­—ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰", default_keywords)
keyword_list = [k.strip() for k in keywords_input.split(",") if k.strip()]

# æŠ“å–æ–°è
if st.button("ğŸ“¥ æŠ“å–æ–°è"):
    with st.spinner("ğŸ”„ æ­£åœ¨æŠ“å–æ–°è..."):
        all_news = []
        for keyword in keyword_list:
            all_news.extend(fetch_news_for_query(keyword))
        st.session_state.news_results = all_news
        st.session_state.form_submitted = False  # æ¯æ¬¡æŠ“å–é‡ç½®è¡¨å–®ç‹€æ…‹
    st.success("âœ… æŠ“å–æˆåŠŸï¼")

# å‹¾é¸è¡¨å–®
if st.session_state.news_results:
    grouped = group_by_label(st.session_state.news_results)

    with st.form("news_selection"):
        for keyword in keyword_list:
            items = grouped.get(keyword, [])
            st.subheader(f"ğŸ”¸ {keyword}")
            if not items:
                st.markdown("ğŸ” ç„¡ç›¸é—œæ–°è")
            for idx, item in enumerate(items):
                key = f"checkbox_{item['label']}_{item['title']}_{idx}"
                label_with_link = f"[{item['title']}]({item['url']})"
                st.checkbox(label_with_link, key=key)

        submitted = st.form_submit_button("ğŸ“¤ ç”¢ç”Ÿ LINE è¨Šæ¯")
        if submitted:
            st.session_state.form_submitted = True

# ç”¢ç”Ÿ LINE è¨Šæ¯
if st.session_state.get('form_submitted', False):

    def get_current_checked_news():
        current_selected_news = []
        for keyword in keyword_list:
            items = grouped.get(keyword, [])
            for idx, item in enumerate(items):
                key = f"checkbox_{item['label']}_{item['title']}_{idx}"
                if st.session_state.get(key, False):
                    current_selected_news.append(item)
        return current_selected_news

    selected_news = get_current_checked_news()

    if not selected_news:
        st.warning("âš ï¸ è«‹è‡³å°‘å‹¾é¸ä¸€å‰‡æ–°è")
    else:
        from collections import defaultdict
        label_group = defaultdict(list)
        for item in selected_news:
            label_group[item['label']].append(item)

        line_msg = "å„ä½é•·å®˜ã€åŒä»æ—©å®‰ï¼Œ\nä»Šæ—¥æ–°èè¼¿æƒ…é€£çµå¦‚ä¸‹ï¼š\n\n"
        for label, items in label_group.items():
            line_msg += f"ã€{label}ã€‘\n"
            for item in items:
                url = item['url']

                if url in st.session_state.resolved_url_cache:
                    resolved_url = st.session_state.resolved_url_cache[url]
                else:
                    parsed_url = urlparse(url)
                    if "news.google.com" in parsed_url.netloc:
                        manager = multiprocessing.Manager()
                        return_dict = manager.dict()
                        p = multiprocessing.Process(target=playwright_worker, args=(url, return_dict))
                        p.start()
                        p.join(timeout=30)
                        if p.is_alive():
                            p.terminate()
                            p.join()
                            resolved_url = url
                        else:
                            resolved_url = return_dict.get('final_url', url)
                    else:
                        resolved_url = url

                    st.session_state.resolved_url_cache[url] = resolved_url

                line_msg += f"{item['title']}\n{resolved_url}\n"
            line_msg += "\n"

        st.success("âœ… è§£æå®Œæˆï¼Œå·²ç”¢ç”Ÿ LINE è¨Šæ¯")
        st.text_area("ğŸ“‹ LINE è¨Šæ¯å…§å®¹ (å¯è¤‡è£½)", line_msg.strip(), height=400, key="line_output")

        js_safe_msg = line_msg.strip().replace('`','\\`').replace('\\','\\\\')
        components.html(f"""
            <button onclick="copyText()" style="font-size:16px;padding:8px 16px;margin-top:10px;">ğŸ“‹ è¤‡è£½åˆ°å‰ªè²¼ç°¿</button>
            <script>
            function copyText() {{
                const text = `{js_safe_msg}`;
                navigator.clipboard.writeText(text).then(function() {{
                    alert("âœ… å·²è¤‡è£½ï¼");
                }}, function(err) {{
                    alert("âŒ å¤±æ•—ï¼š" + err);
                }});
            }}
            </script>
        """, height=70)

